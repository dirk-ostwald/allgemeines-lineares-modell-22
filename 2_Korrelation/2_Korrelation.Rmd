---
fontsize: 8pt
bibliography: 2_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 2_header.tex
---


```{r, include = F}
source("2_R_common.R")
fdir        = file.path(getwd(), "2_Abbildungen")                               # Abbildungsverzeichnis
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("2_Abbildungen/alm_2_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2022

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald


#  {.plain}
\center
\huge
\vfill
\noindent (2) Korrelation
\vfill

#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Partielle Korrelation

Selbstkontrollfragen

\vfill

#
\setstretch{2.2}
\vfill
\large

**Grundlagen**

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Partielle Korrelation

Selbstkontrollfragen

\vfill

# Grundlagen
\large
Anwendungsszenario
\vspace{2mm}

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("2_Abbildungen/alm_2_beispielszenario.pdf")
```

# Grundlagen
Beispieldatensatz

\center
\footnotesize
$i = 1,...,20$ Patient:innen, $y_i$ Symptomreduktion bei Patient:in $i$,  $x_i$ Anzahl Therapiestunden  von Patient:in $i$

\setstretch{1}
```{r, echo = F}
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)
knitr::kable(D, "pipe")
```

# Grundlagen
Beispieldatensatz

```{r, echo = F, eval = F}
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
D$x_i,
D$y_i,
pch        = 16,
xlab       = "Anzahl Therapiestunden (x)",
ylab       = "Symptomreduktion (y)",
xlim       = c(0,21),
ylim       = c(-10, 40))
legend(
"topleft",
TeX("$(x_i,y_i)$"),
lty         = 0,
pch         = 16,
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
dev.copy2pdf(
file        = file.path(fdir, "alm_2_beispieldatensatz.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("2_Abbildungen/alm_2_beispieldatensatz.pdf")
```

\center
\textcolor{darkblue}{Wie stark hängen Anzahl Therapiestunden und Symptomreduktion zusammen?}


# Grundlagen
\small
\begin{definition}[Korrelation]
\justifying
Die \textit{Korrelation} zweier Zufallsvariablen $X$ und $Y$ ist definiert als
\begin{equation}
\rho(X,Y) := \frac{\mathbb{C}(X,Y)}{\mathbb{S}(X)\mathbb{S}(Y)}
\end{equation}
wobei $\mathbb{C}(X,Y)$ die Kovarianz von $X$ und $Y$ und $\mathbb{S}(X)$ und
$\mathbb{S}(Y)$ die Standardabweichungen von $X$ und $Y$, respektive, bezeichnen.
\end{definition}

\footnotesize
Bemerkungen

* $\rho(X,Y)$ wird auch \textit{Korrelationskoeffizient} von $X$ und $Y$ genannt.
* Wir haben bereits gesehen, dass $-1 \le \rho(X,Y) \le 1$ gilt.
* Wenn $\rho(X,Y) = 0$ ist, werden $X$ und $Y$ \textit{unkorreliert} genannt.
* Wir haben bereits gesehen, dass aus der Unabhängigkeit von $X$ und $Y$, folgt dass $\rho(X,Y) = 0$.
* Aus $\rho(X,Y) = 0$  folgt aber wie bereits gesehen die Unabhängigkeit von $X$ und $Y$ im Allgemeinen nicht.


# Grundlagen
\footnotesize
\begin{definition}[Stichprobenkorrelation]
\justifying
$\{(x_1,y_1),...,(x_n,y_n)\} \subset \mathbb{R}$ sei eine Wertemenge. Weiterhin seien:
\begin{itemize}
\item Die Stichprobenmittel der $x_i$ und $y_i$ definiert als
\begin{equation}
\bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
\mbox{ und }
\bar{y} := \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}
\item Die Stichprobenstandardabweichungen $x_i$ und $y_i$ definiert als
\begin{equation}
s_x := \sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i - \bar{x})^2}
\mbox{ und }
s_y := \sqrt{\frac{1}{n-1}\sum_{i=1}^n(y_i - \bar{y})^2}.
\end{equation}
\item Die Stichprobenkovarianz der $(x_1,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
c_{xy} := \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x}_n)(y_i - \bar{y}_n).
\end{equation}
\end{itemize}
Dann ist die \textit{Stichprobenkorrelation} der $(x_1,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
r_{xy} := \frac{c_{xy}}{s_xs_y}
\end{equation}
und  wird auch \textit{Stichprobenkorrelationskoeffizient} genannt.
\end{definition}

# Grundlagen
Beispiel
\vspace{2mm}
\tiny
\setstretch{1.2}
```{r}
# Laden des Beispieldatensatzes
fname = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")     # Dateipfad
D     = read.table(fname, sep = ",", header = TRUE)                              # Laden als Dataframe
x_i   = D$x_i                                                                    # x_i Werte 
y_i   = D$y_i                                                                    # y_i Werte
n     = length(x_i)                                                              # n

# "Manuelle" Berechnung der Stichprobenkorrelation
x_bar = (1/n)*sum(x_i)                                                           # \bar{x}
y_bar = (1/n)*sum(y_i)                                                           # \bar{y}
s_x   = sqrt(1/(n-1)*sum((x_i - x_bar)^2))                                       # s_x
s_y   = sqrt(1/(n-1)*sum((y_i - y_bar)^2))                                       # s_y 
c_xy  = 1/(n-1) * sum((x_i - x_bar) * (y_i - y_bar))                             # c_{xy}
r_xy  = c_xy/(s_x * s_y)                                                         # r_{xy}
print(r_xy)                                                                      # Ausgabe

# Automatische Berechnung mit cor()
r_xy  = cor(x_i,y_i)                                                             # r_{xy}
print(r_xy)                                                                      # Ausgabe
```

\center
\normalsize
$\Rightarrow$ Anzahl Therapiestunden und Symptomreduktion sind hochkorreliert.

# Grundlagen

Mechanik der Kovariationsterme

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("2_Abbildungen/alm_2_korrelationsterme.pdf")
```

\center
\footnotesize
Häufige richtungsgleiche   Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Positive Korrelation

Häufige richtungsungleiche Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Negative Korrelation

Keine häufigen richtungsgleichen oder -entgegengesetzten Abweichungen $\Rightarrow$ Keine Korrelation


# Grundlagen
\vspace{2mm}
Beispiele

```{r, eval = F, echo = F}
library(MASS)                                   
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(3,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.3)


# Modellformulierung
n           = 30                                # Anzahl an Stichprobenvektoren
mu          = c(0,0)                            # Erwartungswertparameter
sigma_1     = 1                                 # \sigma_1  
sigma_2     = 1                                 # \sigma_2
rho_12      = c( 0.8, 0.2, -0.4,                # \rho_12
                 0.6, 0.0, -0.6,
                 0.4,-0.2, -0.8)              

# Datenrealisierung und Visualisierung
set.seed(1)
for(rho in rho_12){
  Sigma  = matrix(c(sigma_1^2             , rho*sigma_1*sigma_2, 
                   rho*sigma_1*sigma_2, sigma_2^2), nrow = 2)
  xy     = mvrnorm(n, mu, Sigma)
  plot(
  xy,
  pch         = 21,
  col         = "white",
  bg          = "black",
  xlab        = TeX("$x$"),
  ylab        = TeX("$y$"),
  xlim        = c(-3,3),
  ylim        = c(-3,3),
  main        = TeX(sprintf("$r_{xy}$ = %.2f", cor(xy[,1],xy[,2]))))
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_korrelationsbeispiele.pdf"),
width       = 11,
height      = 11)
```

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_korrelationsbeispiele.pdf")
```

# Grundlagen
\footnotesize
\begin{theorem}[Stichprobenkorrelation bei linear-affinen Transformationen]
\justifying
\normalfont
Für eine Wertemenge $\{(x_i,y_i)\}_{i = 1,...n} \subset \mathbb{R}^2$ sei
$\{(\tilde{x}_i,\tilde{y}_i)\}_{i = 1,...n} \subset \mathbb{R}^2$ eine linear-affin
transformierte Wertemenge mit
\begin{equation}
(\tilde{x}_i, \tilde{y}_i) = (a_x x_i + b_x, a_y y_i + b_y), a_x,a_y \neq 0.
\end{equation}
Dann gilt
\begin{equation}
|r_{\tilde{x}\tilde{y}}| = |r_{xy}|.
\end{equation}
\end{theorem}
Bemerkungen

* Der Betrag der Stichprobenkorrelation ändert sich bei linear-affiner Datentransformation nicht.
* Man sagt, dass die Stichprobenkorrelation im Gegensatz zur Stichprobenkovarianz \textit{maßstabsunabhängig} ist.


# Grundlagen
\footnotesize
\vspace{1mm}
\underline{Beweis}

Es gilt
\tiny
\begin{align}
\begin{split}
r_{\tilde{x}\tilde{y}} 
& := \frac{\frac{1}{n-1}\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}})(\tilde{y}_i - \bar{\tilde{y}})}
        {\sqrt{\frac{1}{n-1}\sum_{i=1}^n(\tilde{x}_i - \bar{\tilde{x}})^2} \sqrt{\frac{1}{n-1}\sum_{i=1}^n(\tilde{y}_i - \bar{\tilde{y}})^2}}
\\
&  = \frac{\sum_{i=1}^n (a_x x_i + b_x - (a_x\bar{x} + b_x))(a_y y_i + b_y - (a_y \bar{y} + b_y))}
          {\sqrt{\sum_{i=1}^n (a_x x_i + b_x - (a_x \bar{x} + b_x))^2}\sqrt{\sum_{i=1}^n (a_y y_i + b_y - (a_y \bar{y} + b_y))^2}}
\\
&  = \frac{a_x a_y\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
          {\sqrt{a_x^2\sum_{i=1}^n (x_i  - \bar{x})^2}\sqrt{a_y^2\sum_{i=1}^n (y_i - \bar{y})^2}}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}
     \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
          {\sqrt{\sum_{i=1}^n (x_i  - \bar{x})^2}\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}\frac{c_{xy}}{s_x s_y}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}r_{xy}.
\end{split}
\end{align}
\footnotesize
Also folgt, durch Durchspielen aller möglichen Vorzeichenfälle, dass
\begin{equation}
|r_{\tilde{x}\tilde{y}}| = |r_{xy}|.
\end{equation}
$\hfill\Box$

#
\setstretch{2.2}
\vfill
\large

Grundlagen

**Korrelation und Bestimmtheitsmaß**

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und Bestimmtheitsmaß

Überblick
\setstretch{2}
\footnotesize

Das sogenannte Bestimmtheitsmaß $\mbox{R}^2$ ist eine beliebte Statistik.

Numerisch ist $\mbox{R}^2$  das Quadrat des Stichprobenkorrelationskoeffizienten.

Ist die Stichprobenkorrelation  $r_{xy} = 0.5$, dann ist $\mbox{R}^2 = 0.25$, ist $r_{xy} = -0.5$, dann ist $\mbox{R}^2 = 0.25$.

$\Rightarrow \mbox{R}^2$ enthält also weniger Information über die Rohdaten als $r_{xy}$, da das Vorzeichen wegfällt.

$\Rightarrow$ \textit{Perse} ist die Angabe von $\mbox{R}^2$ anstelle von  $r_{xy}$ im Kontext der Korrelation zweier Variablen wenig sinnvoll.

Ein tieferes Verständnis von $\mbox{R}^2$ erlaubt jedoch 

(1) Einen Einstieg in das Konzept von Quadratsummenzerlegungen, einem wichtigen ALM Evaluationsprinzip.
(2) Einen Einstieg in das Verständnis der Zusammenhänge von Ausgleichsgerade und Stichprobenkorrelation.
(3) Einen ersten Einblick in die Tatsache, dass Korrelationen (nur) linear-affine Zusammenhänge quantifizieren.

# Korrelation und Bestimmtheitsmaß
\small
\begin{definition}[Erklärte Werte und Residuen einer Ausgleichsgerade]
\justifying
Gegeben seien eine Wertemenge $\{(x_1,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$
und die zu dieser Wertemenge gehörende Ausgleichsgerade
\begin{equation}
f_{\hat{\beta}} : \mathbb{R} \to \mathbb{R}, x \mapsto f_{\hat{\beta}}(x) := \hat{\beta}_0 + \hat{\beta}_1x
\end{equation}
Dann werden für $i = 1,...,n$
\begin{equation}
\hat{y}_i := \hat{\beta}_0 + \hat{\beta}_1x_i
\end{equation}
die durch die Ausgleichsgerade \textit{erklärten Werte} genannt und
\begin{equation}
\hat{\varepsilon}_i := y_i - \hat{y}_i
\end{equation}
die \textit{Residuen} der Ausgleichsgerade genannt.
\end{definition}

# Korrelation und Bestimmtheitsmaß
Erklärte Werte und Residuen

```{r, echo = F, eval = F}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Ausgleichsgeradenparameter
x_bar       = mean(D$x_i)               # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)               # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)         # Stichprobenkovarianz der (x_i,y_i)-Werte
beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter


# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40))
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")
points(
D$x_i,
beta_0_hat + beta_1_hat*D$x_i,
pch         = 16,
col         = "grey")
arrows(
x0        = D$x_i,
y0        = D$y_i,
x1        = D$x_i,
y1        = beta_0_hat + beta_1_hat*D$x_i,
length    = 0,
col       = "grey")
points(
mean(D$x_i),
mean(D$y_i),
pch      = 16,
col      = "Blue")

dev.copy2pdf(
file        = file.path(fdir, "alm_2_erklaertewerte_residuen.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("2_Abbildungen/alm_2_erklaertewerte_residuen.pdf")
```

\vspace{-5mm}
\center
$\bullet \, (x_i, y_i)$                                     \hspace{2mm}
\textcolor{blue}{$\bullet$} $(\bar{x},\bar{y})$             \hspace{2mm}
\textbf{---} $f_{\hat{\beta}}(x)$                           \hspace{2mm}
\textcolor{lightgray}{$\bullet$} $\hat{y}_i$                \hspace{2mm}
\textcolor{lightgray}{\textbf{---}} $\hat{\varepsilon}_i$   \hspace{2mm}
$i = 1,...,n$

# Korrelation und Bestimmtheitsmaß
\small
\begin{theorem}[Quadratsummenzerlegung bei Ausgleichsgerade]
\justifying
\normalfont
Für eine Wertemenge $\{(x_1,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ und ihre
zugehörige Ausgleichsgerade $f_{\hat{\beta}}$ seien für
\begin{equation}
\bar{y} := \frac{1}{n}\sum_{i=1}^n y_i \mbox{ und }
\hat{y}_i := \hat{\beta}_0 + \hat{\beta}_1x_i, \mbox{ für } i= 1,...,n
\end{equation}
das Stichprobenmittel der $y$-Werte und die durch die Ausgleichsgerade erklärten Werte,
respektive. Weiterhin seien

\center
\vspace{1mm}
\begin{tabular}{ll}
$\mbox{SQT} := \sum_{i = 1}^n (y_i - \bar{y})^2$          & die \textit{Total Sum of Squares}      \\\\
$\mbox{SQE} := \sum_{i = 1}^n (\hat{y}_i - \bar{y})^2$    & die \textit{Explained Sum of Squares}  \\\\
$\mbox{SQR} := \sum_{i = 1}^n (y_i - \hat{y}_i)^2$        & die \textit{Residual Sum of Squares}   \\
\end{tabular}
\vspace{1mm}
\flushleft
Dann gilt
\begin{equation}
\mbox{SQT} = \mbox{SQE} + \mbox{SQR}
\end{equation}
\end{theorem}

# Korrelation und Bestimmtheitsmaß
\footnotesize
\setstretch{1.4}
Bemerkungen

\begin{itemize}
\item SQT repräsentiert die Gesamtstreung der $y_i$-Werte um ihren Mittelwert $\bar{y}$.
\item SQE repräsentiert die Streuung der erklärten Werte $\hat{y}_i$ um ihren Mittelwert
\item[] $\Rightarrow$ Große Werte von SQE repräsentieren eine große absolute Steigung der $y_i$ mit den $x_i$
\item[] $\Rightarrow$ Kleine Werte von SQE repräsentieren eine kleine absolute Steigung der $y_i$ mit den $x_i$
\item SQE ist also ein Maß für die Stärke des linearen Zusammenhangs der $x$- und $y$-Werte
\item SQR ist die Summe der quadrierten Residuen, es gilt
\begin{equation}
\mbox{SQR} := \sum_{i = 1}^n (y_i - \hat{y}_i)^2 := \sum_{i=1}^n \hat{\varepsilon}_i^2
\end{equation}
\item[] $\Rightarrow$ Große Werte von SQR repräsentieren große Abweichungen der erklärten von den beobachteten $y$-Werten
\item[] $\Rightarrow$ Kleine Werte von SQR repräsentieren geringe Abweichungen der erklärten von den beobachteten $y$-Werten
\item SQR ist also ein Maß für die Güte der Beschreibung der Datenmenge durch die Ausgleichsgerade.
\end{itemize}

# Korrelation und Bestimmtheitsmaß
\vspace{2mm}
\footnotesize
\setstretch{1}
\underline{Beweis}
\begin{align}
\begin{split}
\mbox{SQT}
& = \sum_{i=1}^n (y_i - \bar{y})^2 \\
& = \sum_{i=1}^n (y_i - \hat{y}_i  + \hat{y}_i - \bar{y})^2 \\
& = \sum_{i=1}^n ((y_i - \hat{y}_i)  + (\hat{y}_i - \bar{y}))^2 \\
& = \sum_{i=1}^n \left((y_i - \hat{y}_i)^2  + 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + (\hat{y}_i - \bar{y})^2\right) \\
& = \sum_{i=1}^n(\hat{y}_i - \bar{y})^2  + 2\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})  + \sum_{i=1}^n (y_i - \hat{y}_i)^2\\
& = \mbox{SQE}  + 2\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})  + \mbox{SQR} \\
& = \mbox{SQE} + \mbox{SQR}
\end{split}
\end{align}

# Korrelation und Bestimmtheitsmaß
\footnotesize
\underline{Beweis (fortgeführt)}

Dabei ergibt sich die letzte Gleichung mit
\begin{equation}
\bar{\hat{y}}
:= \frac{1}{n}\sum_{i=1}^n \hat{y}_i
= \frac{1}{n}\sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1x_i)
= \hat{\beta}_0 + \hat{\beta}_1\bar{x}
= \bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1\bar{x}
= \bar{y}
\end{equation}
und damit auch
\begin{equation}
\bar{\hat{y}} = \bar{y}
\Leftrightarrow
\frac{1}{n}\sum_{i=1}^n\hat{y}_i = \frac{1}{n}\sum_{i=1}^n y_i
\Leftrightarrow
\sum_{i=1}^n\hat{y}_i = \sum_{i=1}^n y_i
\Leftrightarrow
\bar{y}\sum_{i=1}^n\hat{y}_i = \bar{y}\sum_{i=1}^n y_i
\end{equation}
sowie
\begin{equation}
\bar{\hat{y}} = \bar{y}
\Leftrightarrow
\frac{1}{n}\sum_{i=1}^n\hat{y}_i = \frac{1}{n}\sum_{i=1}^n y_i
\Leftrightarrow
\sum_{i=1}^n y_i = \sum_{i=1}^n\hat{y}_i
\Leftrightarrow
\sum_{i=1}^n y_i \hat{y}_i = \sum_{i=1}^n\hat{y}_i\hat{y}_i
\end{equation}
aus

# Korrelation und Bestimmtheitsmaß
\footnotesize
\underline{Beweis (fortgeführt)}
\begin{align}
\begin{split}
\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})
& = \sum_{i=1}^n (y_i\hat{y}_i - y_i\bar{y} - \hat{y}_i\hat{y}_i + \hat{y}_i\bar{y}) \\
& = \sum_{i=1}^n y_i\hat{y}_i - \sum_{i=1}^n y_i\bar{y} - \sum_{i=1}^n \hat{y}_i\hat{y}_i + \sum_{i=1}^n \hat{y}_i\bar{y} \\
& = \sum_{i=1}^n y_i\hat{y}_i - \sum_{i=1}^n \hat{y}_i\hat{y}_i + \bar{y}\sum_{i=1}^n \hat{y}_i - \bar{y}\sum_{i=1}^n y_i \\
& = 0 + 0 \\
& = 0
\end{split}
\end{align}
$\hfill\Box$


# Korrelation und Bestimmtheitsmaß
\footnotesize
\begin{definition}[Bestimmtheitsmaß $\mbox{R}^2$]
\justifying
Für eine Wertemenge $\{(x_1,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ und ihre
zugehörige Ausgleichsgerade $f_{\hat{\beta}}$ sowie die zugehörigen Explained Sum of Squares $\mbox{SQE}$
und Total Sum of Squares $\mbox{SQT}$ heißt
\begin{equation}
\mbox{R}^2 := \frac{\mbox{SQE}}{\mbox{SQT}}
\end{equation}
\textit{Bestimmtheitsmaß} oder \textit{Determinationskoeffizient}.
\end{definition}

\begin{theorem}[Stichprobenkorrelation und Bestimmtheitsmaß]
\justifying
\normalfont
Für eine Wertemenge $\{(x_1,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ sei
$\mbox{R}^2$ das Bestimmtheitsmaß und $r_{xy}$ sei die Stichprobenkorrelation.
Dann gilt
\begin{equation}
\mbox{R}^2 = r_{xy}^2.
\end{equation}
\end{theorem}


# Korrelation und Bestimmtheitsmaß
\footnotesize

\setstretch{1.8}
Bemerkungen
\begin{itemize}
\item  Mit $-1 \le r_{xy} \le 1$ folgt aus dem Theorem direkt, dass $0 \le \mbox{R}^2 \le 1$.
\item  Es gilt $\mbox{R}^2 = 0$ genau dann, wenn $\mbox{SQE} = 0$ ist
\item[] $\Rightarrow$ Für $\mbox{R}^2 = 0$ ist die erklärte Streuung der Daten durch die Ausgleichsgerade gleich null.
\item[] $\Rightarrow$$\mbox{R}^2 = 0$ beschreibt also den Fall einer denkbar schlechten Erklärung der Daten durch die Ausgleichsgerade.
\item Es gilt $\mbox{R}^2 = 1$ genau dann, wenn $\mbox{SQE} = \mbox{SQT}$ ist.
\item[] $\Rightarrow$ Für $\mbox{R}^2 = 0$ ist also die Gesamtstreuung gleich der durch die Ausgleichsgerade erklärten Streuung.
\item[] $\Rightarrow$ $\mbox{R}^2 = 1$ beschreibt also den Fall das sämtliche Datenvariabilität durch die Ausgleichsgerade erklärt wird.
\end{itemize}

# Korrelation und Bestimmheitsmaß
\vspace{2mm}
\footnotesize
\underline{Beweis}

Wir halten zunächst fest, dass mit
\begin{equation}
\bar{\hat{y}}
:= \frac{1}{n}\sum_{i=1}^n \hat{y}_i
= \frac{1}{n}\sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1x_i)
= \hat{\beta}_0 + \hat{\beta}_1\bar{x}
= \bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1\bar{x}
= \bar{y}
\end{equation}
folgt, dass
\begin{align}
\begin{split}
\mbox{SQE}
& = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2            \\
& = \sum_{i=1}^n (\hat{y}_i - \bar{\hat{y}})^2      \\
& = \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1x_i - \hat{\beta}_0 - \hat{\beta}_1 \bar{x})^2      \\
& = \sum_{i=1}^n (\hat{\beta}_1(x_i - \bar{x}))^2      \\
& = \hat{\beta}_1^2\sum_{i=1}^n (x_i - \bar{x})^2      \\
\end{split}
\end{align}

# Korrelation und Bestimmheitsmaß
\vspace{2mm}
\footnotesize
\underline{Beweis}

Damit ergibt sich dann
\begin{align}
\begin{split}
\mbox{R}^2
& = \frac{\mbox{SQE}}{\mbox{SQT}}                                                 \\
& = \frac{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \hat{\beta}_1^2\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \frac{c_{xy}^2}{s_x^4} \frac{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}{\frac{1}{n-1}\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \frac{c_{xy}^2}{s_x^4} \frac{s_x^2}{s_y^2} \\
& = \frac{c_{xy}^2}{s_x^2s_y^2} \\
& = \left(\frac{c_{xy}}{s_xs_y}\right)^2 \\
& = r_{xy}^2.
\end{split}
\end{align}
$\hfill\Box$


# Korrelation und Bestimmheitsmaß

```{r, eval = F, echo = F}
library(MASS)                                   
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(3,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)


# Modellformulierung
n           = 30                                # Anzahl an Stichprobenvektoren
mu          = c(0,0)                            # Erwartungswertparameter
sigma_1     = 1                                 # \sigma_1  
sigma_2     = 1                                 # \sigma_2
rho_12      = c( 0.8, 0.2, -0.4,                # \rho_12
                 0.6, 0.0, -0.6,
                 0.4,-0.2, -0.8)              

# Datenrealisierung, Datenanalyse und Visualisierung
set.seed(1)
for(rho in rho_12){
  
  # Datenrealisierung
  Sigma  = matrix(c(sigma_1^2             , rho*sigma_1*sigma_2, 
                   rho*sigma_1*sigma_2, sigma_2^2), nrow = 2)
  xy     = mvrnorm(n, mu, Sigma)
  x      = xy[,1]
  y      = xy[,2]
    
  # Ausgleichsgeradenparameter
  x_bar       = mean(x)                   # Stichprobenmittel der x_i-Werte
  y_bar       = mean(y)                   # Stichprobenmittel der y_i-Werte
  s2x         = var(x)                    # Stichprobenvarianz der  x_i-Werte
  cxy         = cov(x,y)                  # Stichprobenkovarianz der (x_i,y_i)-Werte
  beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
  beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter
  
  # Visualisierung
  plot(
  xy,
  pch         = 21,
  col         = "white",
  bg          = "black",
  xlab        = TeX("$x$"),
  ylab        = TeX("$y$"),
  xlim        = c(-3,3),
  ylim        = c(-3,3),
  main        = TeX(sprintf("$r_{xy}$ = %.2f, $R^2$ = %.2f", cor(x,y),cor(x,y)^2)))
  abline(
  coef        = c(beta_0_hat, beta_1_hat),
  lty         = 1,
  col         = "black")
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_r2beispiele.pdf"),
width       = 11,
height      = 11)
```
\vspace{2mm}
Beispiele
```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_r2beispiele.pdf")
```

#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

**Korrelation und lineare Abhängigkeit**

Korrelation und Regression

Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und lineare Abhängigkeit

```{r, eval = F, echo = F}
library(MASS)                                   
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1.1,
cex.main    = 1.2,
xpd         = TRUE)

# Modellformulierung und Datenrealisierungen
set.seed(1)
n           = 40
x           = seq(-pi,pi,len = n)
eps         = rnorm(n)
y_all       = list(x + eps ,x^2 + eps, 8*cos(2*x) + eps)

# Visualisierung
for(y in y_all){
 plot(
 x,
 y,
 pch   = 21,
 bg    = "black",
 col   = "white",
 xlim  = c(-3.5,3.5),
 main  = TeX(sprintf("$r_{xy}$ = %.2f", cor(x,y))))
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_rlinearitaet.pdf"),
width       = 11,
height      = 4)
```
Funktionale Abhängigkeiten und Stichprobenkorrelation

\vspace{1cm}

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("2_Abbildungen/alm_2_rlinearitaet.pdf")
```
\vspace{-5mm}

$\,$
\hspace{1cm}
$y_i = x_i + \varepsilon_i$
\hspace{1.9cm}
$y_i = x_i^2 + \varepsilon_i$
\hspace{1.2cm}
$y_i = 8 \cos(2x_i) + \varepsilon_i$

\center
$\quad\,\,\,\varepsilon_i \sim N(0,1)$


# Korrelation und lineare Abhängigkeit
\small
\begin{theorem}[Korrelation und linear-affine Abhängigkeit]
\justifying
\normalfont
$X$ und $Y$ seien zwei Zufallsvariablen mit positiver Varianz.  Dann besteht genau
dann eine lineare-affine Abhängigkeit der Form
\begin{equation}
Y = \beta_0 + \beta_1X \mbox{ mit } \beta_0,\beta_1\in \mathbb{R}
\end{equation}
zwischen $X$ und $Y$, wenn
\begin{equation}
\rho(X,Y) = 1 \mbox{ oder } \rho(X,Y) = -1
\end{equation}
gilt.
\end{theorem}

\footnotesize
Bemerkungen

* Die lineare Abhängigkeit $Y = \beta_0 + \beta_1X$  impliziert eine lineare Abhängigkeit $X = \tilde{\beta}_0 + \tilde{\beta}_1Y$, denn
\begin{equation}
Y = \beta_0 + \beta_1X
\Leftrightarrow
-\beta_0 + Y = \beta_1X
\Leftrightarrow
X = -\frac{\beta_0}{\beta_1} + \frac{1}{\beta_1}Y
\Leftrightarrow
X = \tilde{\beta}_0 + \tilde{\beta}_1 Y
\end{equation}
mit
\begin{equation}
\tilde{\beta}_0 = -\frac{\beta_0}{\beta_1} \mbox{ und } \tilde{\beta}_1 = \frac{1}{\beta_1}.
\end{equation}

# Korrelation und lineare Abhängigkeit
\footnotesize
\underline{Beweis}
\setstretch{1.0}

Wir beschränken uns auf den Beweis der Aussage, dass aus $Y = \beta_0 + \beta_1 X$
folgt, dass $\rho(X,Y) = \pm 1$ ist. Dazu halten wir zunächst fest, dass mit den
Theoremen zu den Eigenschaften von Erwartungswert und Varianz gilt, dass
\begin{equation}
\mathbb{E}(Y) = \beta_0 + \beta_1\mathbb{E}(X)
\mbox{ und }
\mathbb{V}(Y) = \beta_1^2 \mathbb{V}(X).
\end{equation}
Wegen $\mathbb{V}(X) > 0$ und  $\mathbb{V}(Y) > 0$ gilt damit $\beta_1 \neq 0$.
Es folgt dann
\begin{equation}
\beta_1 > 0 \Rightarrow \mathbb{S}(Y) = \beta_1 \mathbb{S}(X) > 0
\mbox{ und }
\beta_1< 0 \Rightarrow \mathbb{S}(Y) = -\beta_1 \mathbb{S}(X) > 0.
\end{equation}
Weiterhin gilt
\begin{align}
\begin{split}
Y - \mathbb{E}(Y)
& = \beta_0 + \beta_1X - \mathbb{E}(Y)                      \\
& = \beta_0 + \beta_1X - \beta_0 - \beta_1\mathbb{E}(X)     \\
& = \beta_1X - \beta_1\mathbb{E}(X)                         \\
& = \beta_1(X -\mathbb{E}(X)).
\end{split}
\end{align}
Für die Kovarianz von $X$ und $Y$ ergibt sich also
\begin{align}
\begin{split}
\mathbb{C}(X,Y)
& = \mathbb{E}\left((Y - \mathbb{E}(Y))(X - \mathbb{E}(X))\right) \\
& = \mathbb{E}\left(\beta_1(X - \mathbb{E}(X))(X - \mathbb{E}(X))\right) \\
& = \beta_1\mathbb{E}\left((X - \mathbb{E}(X))^2\right) \\
& = \beta_1\mathbb{V}(X).
\end{split}
\end{align}
Damit ergibt für die Korrelation von $X$ und $Y$
\begin{equation}
\rho(X,Y)
= \frac{\mathbb{C}(X,Y)}{\mathbb{S}(X)\mathbb{S}(Y)}
= \pm \frac{\beta_1\mathbb{V}(X)}{\mathbb{S}(X)\beta_1 \mathbb{S}(X)}
= \pm \frac{\beta_1\mathbb{V}(X)}{\beta_1\mathbb{V}(X)}
= \pm 1.
\end{equation}


#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und lineare Abhängigkeit

**Korrelation und Regression**

Korrelation und Bestimmtheitsmaß

Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und Regression
Überblick

\footnotesize
Der fundamentale Unterschied zwischen "Korrelation" und "Regression" ist, dass

* bei Korrelation sowohl die UV (die $x$'s) als auch die AV (die $y$'s) als Zufallsvariablen modelliert werden,
* bei Regression dagegen lediglich die AV als Zufallsvariable modelliert wird und die UV als vorgegeben gilt.

Dieser Tatsache unbenommen, kann man auf gegebene Daten prinzipiell natürlich sowohl "Korrelation" als auch "Regression" anwenden.
Das Ergebnis einer Regressionsanalyse lässt sich in das Ergebnis einer Korrelationsanalyse umrechnen. Die zusätzlich Durchführung einer 
Korrelationsanalyse bei durchgeführter Regressionsanalyse erzeugt kein mehr an Information oder Verständnis über den
Zusammenhang von UV und AV.

Für ein tieferes Verständnis dieser Zusammenhänge ist ein Regressionsmodell nötig, indem auch die UV eine Zufallsvariable ist.
In Abgrenzung zum Modell der einfachen linearen Regression, in dem die UV keine Zufallsvariable ist, bezeichnen wir 
dieses Modell als \textit{Regression}. Letztlich gerät die Terminologie hier an eine Grenze und es muss jeweils geprüft bzw.
geschlossen werden, welches Modell Datenanalysten nun tatsächlich vorschwebt.


# Korrelation und Regression
\small
\begin{definition}[Regressionsgerade zweier Zufallsvariablen]
\justifying
$X$ und $Y$ seien zwei Zufallsvariablen. Dann heißt
\begin{equation}
Y  = \beta_0 + \beta_1 X \mbox{ mit }
\end{equation}
mit
\begin{equation}
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)} \mbox{ und }\beta_0 := \mathbb{E}(Y) - \beta_1\mathbb{E}(X)
\end{equation}
die \textit{Regressionsgerade der Zufallsvariablen $X$ auf $Y$}, $\beta_0$ und
$\beta_1$ heißen die zugehörigen \textit{Regressionskoeffizienten}, und die
Zufallsvariable
\begin{equation}
E := Y - \beta_0 - \beta_1 X
\end{equation}
heißt die \textit{Residualvariable}.
\end{definition}

\footnotesize
Bemerkungen

* $X$ und $Y$ sind Zufallsvariablen, $\beta_0$ und $\beta_1$ sind keine Zufallsvariablen.


# Korrelation und Regression
\small
\begin{theorem}[Optimalität der Regressionsgerade zweier Zufallsvariablen]
\justifying
\normalfont
Unter allen Geraden der Form
\begin{equation}
Y  = \beta_0 + \beta_1 X
\end{equation}
ist die Gerade mit
\begin{equation}
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)} \mbox{ und } \beta_0 :=  \mathbb{E}(Y) - \beta_1\mathbb{E}(X)
\end{equation}
diejenige, für die
\begin{equation}
\tilde{q}: \mathbb{R}^2 \to \mathbb{R}, (\beta_0, \beta_1) \mapsto \tilde{q}(\beta_0,\beta_1) := \mathbb{E}\left((Y - (\beta_0 + \beta_1 X)^2\right)
\end{equation}
ein Minimum hat.
\end{theorem}


# Korrelation und Regression
\tiny
\setstretch{1}
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\underline{Beweis}

Wir halten zunächst fest, dass
\begin{align}
\begin{split}
\tilde{q}(\beta_0,\beta_1)
& = \mathbb{E}\left(Y - \beta_0 - \beta_1 X \right) \\
& = \mathbb{E}\left(Y - \beta_1 X - \beta_0 + \beta\mathbb{E}(X) - \beta_1\mathbb{E}(X) + \mathbb{E}(Y) - \mathbb{E}(Y) \right) \\
& = \mathbb{E}\left((Y - \mathbb{E}(Y)) - \beta_1(X - \mathbb{E}(X)) + (\mathbb{E}(Y) - \beta_1\mathbb{E}(X) - \beta_0)\right) \\
\end{split}
\end{align}
Ausmultiplizieren und Anwendung des Theorems zu den Eigenschaften des Erwartungswerts ergibt dann
\begin{equation}
\tilde{q}(\beta_0,\beta_1) = \mathbb{V}(Y) + \beta_1^2 \mathbb{V}(X) - 2 \beta_1 \mathbb{C}(X,Y) + \left(\mathbb{E}(Y) - \beta_1 \mathbb{E}(X) - \beta_0\right)^2
\end{equation}
Berechnen der partiellen Ableitungen von $\tilde{q}$ hinsichtlich von $\beta_0$ und $\beta_1$ ergibt dann
\begin{equation}\label{eq:partial_beta_0}
\frac{\partial}{\partial \beta_0}\tilde{q}(\beta_0,\beta_1) = -2\left(\mathbb{E}(Y) - \beta_1 \mathbb{E}(X) - \beta_0\right)
\end{equation}
und
\begin{equation}
\frac{\partial}{\partial \beta_1}\tilde{q}(\beta_0,\beta_1) = 2\beta_1\mathbb{V}(X) - 2\mathbb{C}(X,Y) - 2\mathbb{E}(X)\left(\mathbb{E}(Y) - \beta_1\mathbb{E} - \beta_0\right)
\end{equation}
Nullsetzen von \eqref{eq:partial_beta_0} ergibt dann als notwendige Bedingungen für ein Minimum von $\tilde{q}$
\begin{align}
\begin{split}
\frac{\partial}{\partial \beta_0}\tilde{q}(\beta_0^*,\beta_1^*)     = 0
& \Leftrightarrow
\mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) - \beta_0^* = 0 \\
\frac{\partial}{\partial \beta_1}\tilde{q}(\beta_0^*,\beta_1*)  = 0
& \Leftrightarrow
2\beta_1^*\mathbb{V}(X) - 2\mathbb{C}(X,Y) - 2\mathbb{E}(X)\left(\mathbb{E}(Y) - \beta_1^*\mathbb{E} - \beta_0^*\right) = 0
\end{split}
\end{align}
Die erste Gleichung impliziert dann für die zweite Gleichung, dass
\begin{equation}
2\beta_1^*\mathbb{V}(X) - 2\mathbb{C}(X,Y) = 0 \Leftrightarrow \beta_1^* = \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}
\end{equation}
Einsetzen in die erste Gleichung ergibt dann
\begin{align}
\begin{split}
\mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) - \beta_0^* & = 0
\Leftrightarrow
\beta_0^*   = \mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) \\
\end{split}
\end{align}

# Korrelation und Regression
\footnotesize
\begin{theorem}[Zusammenhang von Korrelation und Regression]
\normalfont
\justifying
$X$ und $Y$ seien zwei Zufallsvariablen,
\begin{equation}
Y = \beta_0 + \beta_1 X
\mbox{ mit }
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}
\mbox{ und }
\beta_0 := \mathbb{E}(Y) -\tilde{\beta}_1\mathbb{E}(X)
\end{equation}
sei die Regressionsgerade der Zufallsvariablen $Y$ bezüglich der Zufallsvariablen
$X$ mit den Regressionskoeffizienten  $\beta_0$ und $\beta_1$ und
\begin{equation}
X = \tilde{\beta}_0 + \tilde{\beta_1} Y
\mbox{ mit }
\tilde{\beta}_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(Y)}
\mbox{ und }
\tilde{\beta}_0 := \mathbb{E}(X) -\tilde{\beta}_1\mathbb{E}(Y)
\end{equation}
sei die Regressionsgerade der Zufallsvariablen $X$ bezüglich der Zufallsvariablen
$Y$ mit den Regressionskoeffizienten $\tilde{\beta}_0$ und $\tilde{\beta}_1$. Dann gilt
\begin{equation}
\beta_1 \tilde{\beta}_1
= \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}\frac{\mathbb{C}(X,Y)}{\mathbb{V}(Y)}
= \frac{\mathbb{C}(X,Y)^2}{\mathbb{V}(X)\mathbb{V}(Y)}
= \rho(X,Y)^2.
\end{equation}
\end{theorem}

Bemerkungen

* $\rho(X,Y)$ kann aus den Regressionskoeffizienten von $X$ auf $Y$ und von $Y$ auf $X$ errechnet werden.

# Korrelation und Regression
\footnotesize
\begin{definition}[Stichprobenregressionsgerade]
\justifying
$(X_1,Y_1), ..., (X_n,Y_n)$ sei eine Stichprobe von zweidimensionale Zufallsvektoren
mit identischen unabhängigen Verteilungen. Weiterhin sei für $i = 1,...,n$
\begin{equation}
Y_1 = \beta_0 + \beta_1 X_1
\mbox{ mit }
\beta_1 := \frac{\mathbb{C}(X_1,Y_1)}{\mathbb{V}(X_1)}
\mbox{ und }
\beta_0 := \beta_1\mathbb{E}(X_1) + \mathbb{E}(Y_1)
\end{equation}
die Regressionsgerade der Zufallsvariablen $Y_1$ bezüglich der Zufallsvariablen
$X_1$ mit den Regressionskoeffizienten  $\beta_0$ und $\beta_1$. Schließlich seien
\begin{itemize}
\item $\bar{x}$  und $\bar{y}$ die Stichprobenmittel von Realisierungen der Komponenten der Stichprobe,
\item $s_X^2$ und $s_Y^2$ die Stichprobenvarianzen von Realisierungen der Komponenten der Stichprobe und
\item $c_{X,Y}$ die Stichprobenkovarianz  von Realisierungen der Stichprobe.
\end{itemize}
Dann heißt für $x \in \mathbb{R}$
\begin{equation}
y = b_0 + b_1 x
\mbox{ mit }
b_1 := \frac{c_{X,Y}}{s_X^2}
\mbox{ und }
b_0 := \bar{y} - b_1\bar{x}
\end{equation}
die Regressionsgerade der $y$-Werte bezüglich der $x_i$ Werte in der Stichprobe.
\end{definition}

# Korrelation und Regression

Simulation einer Regressionsgerade

\tiny
\setstretch{1}
```{r}
library(MASS)                                   # multivariate Normalverteilungen

# Modellformulierung
n      = 1e2                                     # Anzahl an Stichprobenvektoren
C_XY   = 1                                      # Kovarianz von X und Y
EX     = 2                                      # Erwartungswert von X
EY     = 1                                      # Erwartungswert von Y
VX     = 2                                      # Varianz von X
VY     = 2                                      # Varianz von Y
beta_1 = C_XY/VX                                # Regressionskoeffizient
beta_0 = -beta_1*EX + EY                        # Regressionskoeffizient

# Realisierungsgeneration
mu     = c(EX, EY)                              # Erwartungswertparameter
Sigma  = matrix(c(VX, C_XY, C_XY,VY), nrow = 2) # Kovarianzmatrixparameter
xy     = mvrnorm(n, mu, Sigma)

# Stichprobenstatistiken
x_bar  = mean(xy[,1])                           # Stichprobenmittel  der x_1,...,x_n
y_bar  = mean(xy[,2])                           # Stichprobenmittel  der y_1,...,y_n
s2X    = var(xy[,1])                            # Stichprobenvarianz der x_1,...,x_n
s2Y    = var(xy[,2])                            # Stichprobenvarianz der y_1,...,y_n
c_xy   = cov(xy[,1],xy[,2])                     # Stichprobenkovarianz

# Stichprobenregressionsgeradenparameter
b_1    = c_xy/s2X                               # Stichprobenregressionskoeffizient
b_0    = -b_1*x_bar + y_bar                     # Stichprobenregressionskoeffizient
```

```{r, echo = F}
cat("beta_0   :"  , beta_0,
    "\nbeta_1   :", beta_1,
    "\nb_0      :"   , b_0,
    "\nb_1      :"   , b_1)
```

```{r, echo = F, eval = F}
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1.2,
cex.main    = 1.2)

# Stichprobe
plot(
xy[,1],
xy[,2],
pch         = 1,
xlim        = c(-2,6),
ylim        = c(-2,6),
xlab        = TeX("$x$"),
ylab        = TeX("y"))

# Regressionsgeraden
abline(coef = c(beta_0, beta_1), lty = 1)
abline(coef = c(b_0, b_1), lty = 2)

legend(
"topleft",
c(TeX("$(x,y)$"),
  TeX("$Y = \\beta_0 + \\beta_1 X$"),
  TeX("$y = b_0 + b_1 x$")),
lty         = c(0, 1, 2),
pch         = c(19,NA,NA),
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
fdir        =  file.path(getwd(), "2_Abbildungen")
dev.copy2pdf(
file        = file.path(fdir, "alm_2_stichprobenregression.pdf"),
width       = 7,
height      = 6)
```

# Korrelation und Regression

Simulation einer Regressionsgerade
\vspace{3mm}

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_stichprobenregression.pdf")
```

#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Korrelation und Bestimmtheitsmaß

**Partielle Korrelation**

Selbstkontrollfragen

\vfill

#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Korrelation und Bestimmtheitsmaß

Partielle Korrelation

**Selbstkontrollfragen**

\vfill



<!-- # Partielle Korrelation -->

<!-- KORRELATION VS KAUSALITÄT -->

<!-- REZIPROZITÄT DER KORRELATIONSKOEFFIZIENTENFORMEL -->


<!-- # Partielle Korrelation -->
<!-- \large -->
<!-- Motivation -->
<!-- \vspace{5mm} -->

<!-- ```{r, echo = FALSE, out.width = "100%"} -->
<!-- knitr::include_graphics("2_Abbildungen/alm_2_partielle_korrelation_1.pdf") -->
<!-- ``` -->

<!-- # Partielle Korrelation -->

<!-- SCHEINKORRELATION AUS PSYCH FORSCHUNGSMETHODEN -->



<!-- # Partielle Korrelation -->

<!-- VERDECKTE KORRELATION AUS PSYCH FORSCHUNGSMETHODEN -->


<!-- # Partielle Korrelation -->

<!-- WAS KANN MAN BLOSS TUN? -->

<!-- # Partielle Korrelation -->

<!-- \small -->
<!-- \begin{definition}[Partielle Korrelation] -->
<!-- \justifying -->
<!-- $X,Y$ und $Z$ seien Zufallsvariablen und -->
<!-- \begin{equation} -->
<!-- E_X := X - \beta_{0,X} - \beta_{1,X}Z -->
<!-- \mbox{ und } -->
<!-- E_Y := Y - \beta_{0,Y} - \beta_{1,Y}Z -->
<!-- \end{equation} -->
<!-- seien die Residualvariablen der Regression von $X$ auf $Z$ und $Y$ auf $Z$, respektive. -->
<!-- Dann ist die \textit{partielle Korrelation von $X$ und $Y$ gegeben $Z$} definiert als -->
<!-- \begin{equation} -->
<!-- \rho_{XY|Z} := \rho(E_X,E_Y) -->
<!-- \end{equation} -->
<!-- \end{definition} -->

<!-- # Partielle Korrelation -->
<!-- \small -->
<!-- \begin{definition}[Partielle Stichprobenkorrelation] -->
<!-- \justifying -->
<!-- $(X_1,Y_1, Z_1), ..., (X_n,Y_n, Z_n)$ sei eine Stichprobe von dreidimensionalen -->
<!-- Zufallsvektoren mit identischen unabhängigen Verteilungen und $(x_1,y_1,z_1) ... -->
<!-- (x_n,y_n,z_n)$ seien Realisation dieser Zufallsvektoren. Weiterhin seien für $i = 1,...,n$ -->
<!-- \begin{equation} -->
<!-- e_i^x = x_i - b_0^x + b_1^x z_i -->
<!-- \mbox{ mit } -->
<!-- b_1^x := \frac{c_{Z,X}}{s_Z^2} -->
<!-- \mbox{ und } -->
<!-- b_0^x := \bar{x} - b_1^x\bar{z} -->
<!-- \end{equation} -->
<!-- und -->
<!-- \begin{equation} -->
<!-- e_i^y = y_i - b_0^y + b_1^y z_i -->
<!-- \mbox{ mit } -->
<!-- b_1^y := \frac{c_{Z,Y}}{s_Y^2} -->
<!-- \mbox{ und } -->
<!-- b_0^y := \bar{y} - b_1^y\bar{z} -->
<!-- \end{equation} -->
<!-- die Werte der entsprechenden Residualvariablen. Dann ist die \textit{partielle -->
<!-- Stichprobenkorrelation} der $x$- und $y$-Werte gegeben die $z$-Werte definiert als -->
<!-- \begin{equation} -->
<!-- r_{xy|z} := \frac{c_{e^x,e^y}}{{s_{e^x},s_{e^y}}}. -->
<!-- \end{equation} -->
<!-- wobei $c_{e^x,e^y}, s_{e^x}$ und $s_{e^y}$ die Stichprobenkovariation der $e^x$ und $e^y$ -->
<!-- sowie deren respektiven Stichprobenstandardabweichungen bezeichnen. -->
<!-- \end{definition} -->

<!-- # Partielle Korrelation -->
<!-- Beispiel -->
<!-- \footnotesize -->

<!-- Es seien -->
<!-- \begin{equation} -->
<!-- z \sim N(\mu_z,\sigma^2_{zz}) -->
<!-- \mbox{ und } -->
<!-- \begin{pmatrix} x \\ y \end{pmatrix} -->
<!-- \sim N(Az,\Sigma_{xy}) -->
<!-- \mbox{ mit } -->
<!-- A:= \begin{pmatrix} 1 \\ 1 \end{pmatrix} -->
<!-- \mbox{ und } -->
<!-- \Sigma := \begin{pmatrix} \sigma_{xx}^2 & \sigma_{xy}^2 \\ \sigma_{xy}^2 & \sigma_{yy}^2 \end{pmatrix}. -->
<!-- \end{equation} -->
<!-- Dann gilt mit dem Theorem zu gemeinsamen Normalverteilungen, dass -->
<!-- \begin{equation} -->
<!-- \begin{pmatrix} z \\ x \\ y \end{pmatrix} -->
<!-- \sim -->
<!-- N\left(\mu,\Sigma\right) -->
<!-- \end{equation} -->
<!-- und -->
<!-- \begin{equation} -->
<!-- \mu = \begin{pmatrix} \mu_z \\ A\mu_z \end{pmatrix} = \begin{pmatrix} \mu_z \\ \mu_z \\ \mu_z \end{pmatrix} -->
<!-- \end{equation} -->
<!-- und -->
<!-- \begin{equation} -->
<!-- \Sigma -->
<!-- = \begin{pmatrix} -->
<!-- \sigma_{zz}^2    & \sigma_{zz}^2 A^T \\ -->
<!-- A\sigma_{zz}^2      & \Sigma_{xy} + \sigma_{zz}^2 AA^T -->
<!-- \end{pmatrix} -->
<!-- = \begin{pmatrix} -->
<!-- \sigma_{zz}^2     & \sigma_{zz}^2                 & \sigma_{zz}^2 \\ -->
<!-- \sigma_{zz}^2     & \sigma_{zz}^2 + \sigma_{xx}^2 & \sigma_{zz}^2 + \sigma_{xx}^2\\ -->
<!-- \sigma_{zz}^2     & \sigma_{zz}^2 + \sigma_{xy}^2 & \sigma_{zz}^2 + \sigma_{yy}^2\\ -->
<!-- \end{pmatrix} -->
<!-- \end{equation} -->

<!-- # Partielle Korrelation -->
<!-- Beispiel -->

<!-- \tiny -->
<!-- \setstretch{1} -->
<!-- ```{r} -->
<!-- # Modellformulierung -->
<!-- library(MASS)                                                          # Multivariate Normalverteilungen -->
<!-- library(matlib)                                                        # Matrizentools -->
<!-- library(ppcor)                                                         # Partielle Korrelationen -->
<!-- n         = 1e5                                                        # Stichprobengröße -->
<!-- m         = 3                                                          # Anzahl an Variablen -->
<!-- mu_z      = 0                                                          # \mu_z -->
<!-- sigsqr_zz = 1                                                          # \sigma_zz^2 -->
<!-- A         = matrix(c(1,1), nrow = 2)                                   # A:= (1,1)^T -->
<!-- Sigma_xy  = matrix(c(.3,.1,                                            # \Sigma_{xy} -->
<!--                      .1,.3), -->
<!--                    nrow = 2, -->
<!--                    byrow = TRUE) -->
<!-- mu        = rbind(mu_z, A %*% mu_z)                                    # \mu_z -->
<!-- Sigma     = rbind(cbind(sigsqr_zz  , sigsqr_zz*t(A)),                  # \Sigma -->
<!--                   cbind(sigsqr_zz*A, Sigma_xy + sigsqr_zz*A%*%t(A))) -->
<!-- iSigma    = inv(Sigma)                                                 # Precision matrix -->
<!-- iS        = inv(diag(sqrt(diag(Sigma))))                                     # inverse Standardabweichungen -->
<!-- Rho       = iS %*% Sigma %*% iS -->

<!-- # Datenrealisierung -->
<!-- zxy       = mvrnorm(n, mu, Sigma)                                      # (z_i,x_i,y_i), i = 1,...,n -->

<!-- # Datenanalyse -->
<!-- z_bar     = mean(zxy[,1])                                              # Stichprobenmittel der z_1,...,z_n -->
<!-- x_bar     = mean(zxy[,2])                                              # Stichprobenmittel der x_1,...,x_n -->
<!-- y_bar     = mean(zxy[,3])                                              # Stichprobenmittel der y_1,...,y_n -->
<!-- s2z       = var(zxy[,1])                                               # Stichprobenvarianzder z_1,...,z_n -->
<!-- c_zx      = cov(zxy[,1],zxy[,2])                                       # Stichprobenkovarianz -->
<!-- c_zy      = cov(zxy[,1],zxy[,2])                                       # Stichprobenkovarianz -->
<!-- b_1x      = c_zx/s2z                                                   # Stichprobenregressionskoeffizient -->
<!-- b_1y      = c_zy/s2z                                                   # Stichprobenregressionskoeffizient -->
<!-- b_0x      = -b_1x*z_bar + y_bar                                        # Stichprobenregressionskoeffizient -->
<!-- b_0y      = -b_1x*z_bar + y_bar                                        # Stichprobenregressionskoeffizient -->
<!-- ex        = zxy[,2] - b_0x - b_1x*zxy[,1]                              # Residualwerte e_X -->
<!-- ey        = zxy[,3] - b_0y - b_1y*zxy[,1]                              # Residualwerte e_Y -->
<!-- r_xy      = cor(zxy[,2], zxy[,3])                                      # Korrelation von X und Y -->
<!-- r_exey    = cor(ex,ey)                                                 # Partielle Korrelation von X und Y -->
<!-- p_xy      = pcor(zxy) -->

<!-- # Ausgabe -->
<!-- cat("r_xy  : ", r_xy, -->
<!--     "\nr_exey: ", r_exey, -->
<!--     "\np_xy  : ", p_xy$estimate[2,3]) -->
<!-- ``` -->

#
\setstretch{2.2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Partielle Korrelation

**Selbstkontrollfragen**

\vfill


# Selbstkontrollfragen
\setstretch{1.8}
\tiny
\justifying

1. Geben Sie die Definition der Korrelation zweier Zufallsvariablen wieder.
2. Geben Sie die Definitionen von Stichprobenmittel, -standardabweichung, -kovarianz und -korrelation wieder.
3. Erläutern Sie anhand der Mechanik der Kovariationsterme, wann eine Stichprobenkorrelation einen hohen absoluten Wert annimmt,
einen hohen positiven Wert annimmt, einen hohen negativen Wert annimmt und einen niedrigen Wert annimmt.
4. Berechnen Sie die Korrelation von Anzahl der Therapiestunden und Symptomreduktion anhand der Daten in Beispieldatensatz.csv.
5. Geben Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen wieder.
6. Erläutern Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen.
7. Geben Sie die Definitionen von erklärten Werten und Residuen einer Ausgleichsgerade wieder.
8. Geben Sie das Theorem zur Quadratsummenzerlegung bei einer Ausgleichsgerade wieder.
9. Erläutern Sie die intuitiven Bedeutungen von $\mbox{SQT}, \mbox{SQE}$ und $\mbox{SQR}$.
10. Geben Sie die Definition des Bestimmtheitsmaßes $\mbox{R}^2$ wieder.
11. Geben Sie das Theorem zum Zusammenhang von Stichprobenkorrelation und Bestimmtheitsmaß wieder. 
12. Erläutern Sie die Bedeutung von hohen und niedrigen  $\mbox{R}^2$ Werten im Lichte der Ausgleichsgerade.
13. Berechnen Sie in einem R-Skript $\mbox{R}^2$ für die Daten in der Datei Beispieldatensatz.csv anhand der Definition von $\mbox{R}^2$. Überprüfen
Sie Ihr Ergebnis anhand des Theorems zum Zusammenhang von Stichprobenkorrelation und Bestimmheitsmaß.
13. Geben Sie das Theorem zum Zusammenhang von Korrelation und linear-affiner Abhängigkeit wieder.
14. Geben Sie die Definition der Regressionsgerade zweier Zufallsvariablen wieder.
15. Geben Sie das Theorem zur Optimalität der Regressionsgerade zweier Zufallsvariablen wieder.
16. Geben Sie das Theorem zum Zusammenhang von Korrelation und Regression an.
17. Erläutern Sie, wie aus den Ergebnissen einer Regressionananlyse das Ergebnis einer Korrelationsanalyse errechnet werden kann.
