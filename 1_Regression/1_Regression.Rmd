---
fontsize: 8pt
bibliography: 1_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 1_header.tex
---


```{r, include = F}
source("1_R_common.R")
fdir        = file.path(getwd(), "1_Abbildungen")                               # Abbildungsverzeichnis
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("1_Abbildungen/alm_1_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2022

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald


#  {.plain}
\center
\huge
\vfill
\noindent (1) Regression
\vfill

#
\setstretch{3}
\vfill
\large

Methode der kleinsten Quadrate

Einfache lineare Regression

Selbstkontrollfragen

\vfill


#
\setstretch{3}
\vfill
\large

**Methode der kleinsten Quadrate**

Einfache lineare Regression

Selbstkontrollfragen

\vfill


# Methode der kleinsten Quadrate
\large
Anwendungsszenario
\vspace{2mm}

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("1_Abbildungen/alm_1_beispielszenario.pdf")
```

# Methode der kleinsten Quadrate
Beispieldatensatz

\center
\footnotesize
$i = 1,...,20$ Patient:innen, $y_i$ Symptomreduktion bei Patient:in $i$,  $x_i$ Anzahl Therapiestunden  von Patient:in $i$

\setstretch{1}
```{r, echo = F}
library(MASS)                                         # Normalverteilungen
set.seed(0)                                           # Ergebnisreproduzierbarkeit
n           = 20                                      # Anzahl Datenpunkte
p           = 3                                       # Anzahl Regressionskoeffizienten
x           = seq(1,n,len = n)                        # Kontrollvariable
X           = matrix(c(rep(1,n), x, x^2), ncol = 3)   # Designmatrix
beta        = matrix(c(.5,.2,.06), ncol = 1)          # Wahre, unbekannte, Regressionskoeffizientenwerte
mu          = X %*% beta                              # Erwartungswertparameter
sigsqr      = 10                                      # Varianzparameter
Sigma       = sigsqr*diag(n)                          # Kovarianzmatrixparameter
y           = as.matrix(mvrnorm(1,mu,Sigma))          # Datengeneration
D           = data.frame(y_i = y, x_i = x)            # Dataframe

# Datensicherung
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
write.csv(D, file = fname, row.names = FALSE)

# Tabelle
knitr::kable(D, "pipe")
```

# Methode der kleinsten Quadrate
Beispieldatensatz

```{r, echo = F, eval = F}
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
x,
y,
pch        = 16,
xlab       = "Anzahl Therapiestunden (x)",
ylab       = "Symptomreduktion (y)",
xlim       = c(0,21),
ylim       = c(-10, 40))

legend(
"topleft",
TeX("$(x_i,y_i)$"),
lty         = 0,
pch         = 16,
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)

dev.copy2pdf(
file        = file.path(fdir, "alm_1_beispieldatensatz.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("1_Abbildungen/alm_1_beispieldatensatz.pdf")
```

\center
\textcolor{darkblue}{Welcher funktionaler Zusammenhang zwischen $x$ und $y$ liegt den Daten zugrunde?}

# Methode der kleinsten Quadrate
\footnotesize
\begin{definition}[Ausgleichsgerade]
\justifying
Für $\beta := (\beta_0,\beta_1)^T \in \mathbb{R}^2$ heißt die linear-affine Funktion
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \beta_0 + \beta_1 x,
\end{equation}
für die für eine Wertemenge  $\{(x_1,y_1),...,(x_n,y_n)\} \subset \mathbb{R}^2$ die Funktion
\begin{equation}
q : \mathbb{R}^2 \to \mathbb{R}_{\ge 0}, \beta \mapsto q(\beta)
:= \sum_{i=1}^n (y_i-f_\beta(x_i))^2
 = \sum_{i=1}^n (y_i- (\beta_0 + \beta_1x_i))^2
\end{equation}
der quadrierten vertikalen Abweichungen der $y_i$ von den Funktionswerten $f_{\beta}(x_i)$
ihr Minimum annimt, die \textit{Ausgleichsgerade für die Wertemenge $\{(x_1,y_1),...,(x_n,y_n)\}$}.
\end{definition}


Bemerkungen

* Wir nehmen hier ohne Beweis an, dass das Minimum von $q$ eindeutig ist.

# Methode der kleinsten Quadrate

Linear-affine Funktionen $f_\beta(x) := \beta_0 + \beta_1 x$

\small
* $\beta_0$: Schnittpunkt von Gerade und $y$-Achse ("Offset Parameter")
* $\beta_1$: $y$-Differenz pro $x$-Einheitsdifferenz ("Steigungsparameter")

\vspace{1cm}

```{r, echo = F, eval = F}
# Ausgleichs- und weitere Geraden
X           = matrix(c(rep(1,n), x), ncol = 2)              # Designmatrix
beta_hat    = inv(t(X) %*% X) %*% t(X) %*% y                # OLS Schätzer
beta_set    = matrix(c(5,.5, -20,3,beta_hat), nrow = 2)     # Weitere Geraden


# Visualisierung
lab         = c(TeX("$\\beta_0 =   5.0, \\beta_1 = 0.5$"),  # Labels
                TeX("$\\beta_0 = -20.0, \\beta_1 = 3.0$"),
                TeX("$\\beta_0 =  -6.2, \\beta_1 = 1.7$"))
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
for(i in 1:3){
  plot(
  x,
  y,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = lab[i])
  abline(coef = beta_set[,i], lty = 1, col = "black")
}
dev.copy2pdf(
file        = file.path(fdir, "alm_1_ausgleichsgerade_1.pdf"),
width       = 12,
height      = 4)
```

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("1_Abbildungen/alm_1_ausgleichsgerade_1.pdf")
```

# Methode der kleinsten Quadrate
\small
Funktion der quadrierten vertikalen Abweichungen
\begin{equation}
q(\beta) := \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2
\end{equation}

```{r, echo = F, eval = F}
# q Funktionswerte
q1          = t(y - X %*% beta_set[,1]) %*% (y - X %*% beta_set[,1])
q2          = t(y - X %*% beta_set[,2]) %*% (y - X %*% beta_set[,2])
q3          = t(y - X %*% beta_set[,3]) %*% (y - X %*% beta_set[,3])

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
lab         = c(TeX("$q(\\beta) = 1159$"),
                TeX("$q(\\beta) = 1451$"),
                TeX("$q(\\beta) = 250$"))

for(i in 1:3){
  plot(
  x,
  y,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = lab[i])
  abline(coef = beta_set[,i], lty = 1, col = "black")
  arrows(
  x0        = x,
  y0        = y,
  x1        = x,
  y1        = X %*% beta_set[,i],
  length    = 0,
  col       = "orange")
}
dev.copy2pdf(
file        = file.path(fdir, "alm_1_ausgleichsgerade_2.pdf"),
width       = 12,
height      = 4)
```

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("1_Abbildungen/alm_1_ausgleichsgerade_2.pdf")
```
\center
\textcolor{orange}{\textbf{------}} $y_i - (\beta_0 + \beta_1x_i)$ für $i = 1,...,n$

# Methode der kleinsten Quadrate
\footnotesize
\begin{theorem}[Ausgleichsgerade]
\justifying
\normalfont
Für eine Wertemenge $\{(x_1,y_1),...,(x_n,y_n)\}\subset\mathbb{R}^2$ hat die Ausgleichsgerade die Form
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \hat{\beta}_0 + \hat{\beta}_1 x,
\end{equation}
wobei mit der Stichprobenkovarianz $c_{xy}$ der $(x_i,y_i)$-Werte, der
Stichprobenvarianz $s_x^2$ der $x_i$-Werte und den Stichprobenmitteln $\bar{x}$
und $\bar{y}$ der $x_i$- und $y_i$-Werte, respektive, gilt, dass
\begin{equation}
\hat{\beta}_1 = \frac{c_{xy}}{s_x^2} \mbox{ und } \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
\end{equation}
\end{theorem}
Bemerkungen

* Mit den Definitionen von $c_{xy}$ und $s_x^2$ gilt also
\begin{equation}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{equation}
* Man spricht hier von der Stichprobenkovarianz $c_{xy}$, auch wenn die Werte $x_1,...,x_n$
oft nicht als Realisierungen einer Stichprobe $X_1,...,X_n$ verstanden werden, sondern 
als gegebene oder selbst gewählte Zahlen.

# Methode der kleinsten Quadrate
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.2}
\underline{Beweis}

Wir betrachten die Summe der quadrierten vertikalen Abweichungen der $y_i$ von
den Funktionswerten $f(x_i)$ als Funktion von $\beta_0$ und $\beta_1$ und bestimmen
Werte $\hat{\beta}_0$ und $\hat{\beta}_1$, für die diese Funktion ihr Minimum annimmt,
die Summe der quadrierten vertikalen Abweichungen der $y_i$ von
den Funktionswerten $f(x_i)$ also minimal ist. Wir betrachten also die Funktion
\begin{equation}
q : \mathbb{R}^2 \to \mathbb{R}, (\beta_0,\beta_1) \mapsto q(\beta_0,\beta_1)
:= \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x)\right)^2.
\end{equation}
Um das Minimum dieser Funktion zu bestimmen, berechnen wir zunächst die partiellen
Ableitungen hinsichtlich $\beta_0$ und $\beta_1$ und setzen diese gleich 0. Es
ergibt sich zunächst
\begin{align}
\begin{split}
\frac{\partial}{\partial \beta_0} q(\beta_0,\beta_1)
& = \frac{\partial}{\partial \beta_0}\left(\sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2\right) \\[-5pt]
& = \sum_{i=1}^n \frac{\partial}{\partial \beta_0} \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2 \\[-5pt]
& = \sum_{i=1}^n 2\left(y_i - (\beta_0 + \beta_1 x_i)\right)\frac{\partial}{\partial \beta_0}\left(y_i - \beta_0 - \beta_1 x_i \right)  \\[-5pt]
&  = -2\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)
\end{split}
\end{align}


# Methode der kleinsten Quadrate
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.2}
\vspace{3mm}
\underline{Beweis (fortgeführt)}

Weiterhin ergibt sich
\vspace{-2mm}
\begin{align}
\begin{split}
\frac{\partial}{\partial \beta_1} q(\beta_0,\beta_1)
& = \frac{\partial}{\partial \beta_1}\left(\sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2\right) \\[-5pt]
& = \sum_{i=1}^n \frac{\partial}{\partial \beta_1} \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2 \\[-5pt]
& = \sum_{i=1}^n 2\left(y_i - (\beta_0 + \beta_1 x_i)\right)\frac{\partial}{\partial \beta_1}\left(y_i - \beta_0 - \beta_1 x_i \right) \\[-5pt]
& = -2\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)x_i
\end{split}
\end{align}
Nullsetzen beider partieller Ableitungen ergibt dann
\begin{align}
\begin{split}
\frac{\partial}{\partial \beta_0} q(\beta_0,\beta_1)  = 0       & \mbox{ und }
\frac{\partial}{\partial \beta_1} q(\beta_0,\beta_1)  = 0
\\[-5pt]
\Leftrightarrow
-2\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)    = 0 & \mbox{ und }
-2\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)x_i = 0
\\[-5pt]
\Leftrightarrow
\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)       = 0 & \mbox{ und }
\sum_{i=1}^n \left(y_i - \beta_0  - \beta_1 x_i\right)x_i    = 0
\end{split}
\end{align}

# Methode der kleinsten Quadrate
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.2}
\underline{Beweis (fortgeführt)}

und weiter
\vspace{-2mm}
\begin{align}
\begin{split}
\sum_{i=1}^n y_i    - \sum_{i=1}^n \beta_0     - \beta_1 \sum_{i=1}^n x_i    = 0 & \mbox{ und }
\sum_{i=1}^n y_ix_i - \sum_{i=1}^n \beta_0x_i  - \beta_1 \sum_{i=1}^n x_i^2  = 0 \\[-5pt]
\Leftrightarrow
\beta_0n  + \beta_1 \sum_{i=1}^n x_i    = \sum_{i=1}^n y_i & \mbox{ und }
\beta_0\sum_{i=1}^n x_i  + \beta_1 \sum_{i=1}^n x_i^2  = \sum_{i=1}^n y_ix_i
\end{split}
\end{align}
Das sich hier ergebende Gleichungssystem
\begin{align}
\begin{split}
\beta_0 n                + \beta_1 \sum_{i=1}^n x_i    & = \sum_{i=1}^n y_i    \\[-5pt]
\beta_0 \sum_{i=1}^n x_i + \beta_1 \sum_{i=1}^n x_i^2  & = \sum_{i=1}^n y_ix_i
\end{split}
\end{align}
wird \textit{System der Normalengleichungen} genannt und beschreibt die notwendige
Bedingung für ein Minimum von $q$. Auflösen dieses Gleichungssystems nach
$\beta_0$ und $\beta_1$ liefert dann die Werte $\hat{\beta}_0$ und $\hat{\beta}_1$
des Theorems. 


# Methode der kleinsten Quadrate
\tiny
\vspace{2mm}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.1}
\underline{Beweis (fortgeführt)}

Um dies zu sehen, halten wir zunächst fest, dass mit der ersten Gleichung des Systems
der Normalengleichungen gilt
\begin{align}
\begin{split}
n\hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^n x_i          = \sum_{i=1}^n y_i                 
\Leftrightarrow \hat{\beta}_0 + \hat{\beta}_1\bar{x}     = \bar{y}                          
\Leftrightarrow \hat{\beta}_0                            = \bar{y} - \hat{\beta}_1\bar{x}   
\end{split}
\end{align}
Einsetzen der Form von $\hat{\beta}_0$ in die zweite Gleichung des Systems der Normalengleichungen
ergibt dann zunächst
\begin{align}\label{eq:hat_beta_1}
\begin{split}
\hat{\beta}_0 \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2                                         
& = \sum_{i=1}^n y_ix_i   
\\
\Leftrightarrow
\left(\bar{y} - \hat{\beta}_1\bar{x}\right) \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2           
& = \sum_{i=1}^n y_ix_i   
\\
\Leftrightarrow
\bar{y}\sum_{i=1}^n x_i - \hat{\beta}_1\bar{x}\sum_{i=1}^n x_i  + \hat{\beta}_1 \sum_{i=1}^n x_i^2           
& = \sum_{i=1}^n y_ix_i   
\\
\Leftrightarrow
- \hat{\beta}_1\bar{x}\sum_{i=1}^n x_i  + \hat{\beta}_1 \sum_{i=1}^n x_i^2           
& = \sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i  
\\
\Leftrightarrow
\hat{\beta}_1 \left(\sum_{i=1}^n x_i^2  - \bar{x}\sum_{i=1}^n x_i\right)         
& = \sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i  
\end{split}
\end{align}

# Methode der kleinsten Quadrate
\tiny
\vspace{2mm}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.1}
\underline{Beweis (fortgeführt)}

Wir halten nun zunächst fest, dass gilt
\begin{align}
\begin{split}
\sum_{i=1}^n x_i^2  - \bar{x}\sum_{i=1}^n x_i
& = \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + \bar{x}\sum_{i=1}^n x_i \\
& = \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n\left(\frac{1}{n}\sum_{i=1}^n x_i\right)\bar{x} \\
& = \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n\bar{x}^2 \\
& = \sum_{i=1}^n \left(x_i^2 - 2\bar{x} x_i + \bar{x}^2\right) \\
& = \sum_{i=1}^n \left(x_i - \bar{x}\right)^2. \\
\end{split}
\end{align}

# Methode der kleinsten Quadrate
\tiny
\vspace{2mm}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.1}
\underline{Beweis (fortgeführt)}

Weiterhin halten wir zunächst fest, dass gilt
\begin{align}
\begin{split}
\sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i 
& = \sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i - n\bar{y}\bar{x} + n\bar{y}\bar{x}                       \\
& = \sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i - \sum_{i=1}^n y_i\bar{x}  + \sum_{i=1}^n\bar{y}\bar{x}   \\
& = \sum_{i=1}^n y_ix_i - \sum_{i=1}^n y_i\bar{x} - \sum_{i=1}^n\bar{y} x_i  + \sum_{i=1}^n\bar{y}\bar{x}   \\
& = \sum_{i=1}^n \left(y_ix_i -  y_i\bar{x} - \bar{y} x_i  + \bar{y}\bar{x}\right)                          \\
& = \sum_{i=1}^n \left(y_i-\bar{y}\right)\left(x_i-\bar{x}\right).
\end{split}
\end{align}

# Methode der kleinsten Quadrate
\tiny
\vspace{2mm}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.1}
\underline{Beweis (fortgeführt)}

In der Fortsetzung von \eqref{eq:hat_beta_1} ergibt sich dann
\begin{align} 
\begin{split}
\hat{\beta}_1 \left(\sum_{i=1}^n x_i^2  - \bar{x}\sum_{i=1}^n x_i\right)         
& = \sum_{i=1}^n y_ix_i - \bar{y}\sum_{i=1}^n x_i  
\\
\Leftrightarrow
\hat{\beta}_1 \left(\sum_{i=1}^n \left(x_i - \bar{x}\right)^2\right)         
& =  \sum_{i=1}^n \left(y_i-\bar{y}\right)\left(x_i-\bar{x}\right) 
\\
\Leftrightarrow
\hat{\beta}_1        
& = \frac{\sum_{i=1}^n \left(y_i-\bar{y}\right)\left(x_i-\bar{x}\right)}{\sum_{i=1}^n \left(x_i - \bar{x}\right)^2} 
\\
\Leftrightarrow
\hat{\beta}_1        
& = \frac{c_{xy}}{s_x^2}. 
\end{split}
\end{align}

$\hfill\Box$



# Methode der kleinsten Quadrate
Beispieldatensatz Analyse
\vspace{1mm}
\setstretch{1.2}


\footnotesize
```{r}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Stichprobenstatistiken
x_bar       = mean(D$x_i)               # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)               # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)         # Stichprobenkovarianz der (x_i,y_i)-Werte

# Ausgleichsgeradenparameter
beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter

# Ausgabe
cat("beta_0_hat:", beta_0_hat,
    "\nbeta_1_hat:", beta_1_hat)
```


# Methode der kleinsten Quadrate
Beispieldatensatz Visualisierung
\vspace{1mm}
\setstretch{1.2}

```{r, echo = F, eval = F}
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Datenwerte
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40),
main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")

# Legende
legend(
"topleft",
c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
lty = c(0,1),
pch = c(16, NA),
bty = "n")

# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_1_ausgleichsgerade_3.pdf"),
width       = 4,
height      = 4)
```

\tiny
```{r, eval = F}
# Datenwerte
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40),
main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")

# Legende
legend(
"topleft",
c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
lty        = c(0,1),
pch        = c(16, NA),
bty        = "n")
```


# Methode der kleinsten Quadrate
Beispieldatensatz Visualisierung
\vspace{5mm}

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("1_Abbildungen/alm_1_ausgleichsgerade_3.pdf")
```

# Methode der kleinsten Quadrate
\footnotesize
\begin{definition}[Ausgleichspolynom]
\justifying
Für $\beta := (\beta_0,...,\beta_k)^T \in \mathbb{R}^{k+1}$ heißt die Polynomfunktion $k$ten Grades
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \sum_{i=0}^k \beta_i x^i,
\end{equation}
für die für eine Wertemenge  $\{(x_1,y_1),...,(x_n,y_n)\} \subset \mathbb{R}^2$ die Funktion
\begin{equation}
q : \mathbb{R}^2 \to \mathbb{R}_{\ge 0}, \beta \mapsto q(\beta)
:= \sum_{i=1}^n \left(y_i-f_\beta(x_i)\right)^2
 = \sum_{i=1}^n \left(y_i- \sum_{i=0}^k \beta_i x^i\right)^2
\end{equation}
der quadrierten vertikalen Abweichungen der $y_i$ von den Funktionswerten
$f_{\beta}(x_i)$ ihr Minimum annimt, das \textit{Ausgleichspolynom $k$ten Grades}
für die Wertemenge $\{(x_1,y_1),...,(x_n,y_n)\}$.
\end{definition}

\footnotesize
Bemerkungen

* Wir nehmen hier ohne Beweis an, dass das Minimum von $q$ eindeutig ist.
* Die Ausgleichsgerade ist das Ausgleichspolynom ersten Grades.
* Die Paramterwerte $\hat{\beta}_0,...,\hat{\beta}_k$ für die $q$ bei gegebener Wertemenge
  ihr Minimum annehmen werden an späterer Stelle im Rahmen der Theorie des
  Allgemeinen Linearen Modells ALMs bestimmt werden.


# Methode der kleinsten Quadrate
\small
Beispieldatensatz Ausgleichspolynome 1ten bis 4ten Grades

\footnotesize
```{r, echo = F, eval = F}
# Daten und Modellparameter
library(matlib)                                                                  # Matrizentools
library(pracma)                                                                  # Polynomtools
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")                  # Datendatei
D           = read.table(fname, sep = ",", header = TRUE)                        # Dataframe
y           = D$y_i                                                              # y_i Werte
x           = D$x_i                                                              # x_i Werte
n           = length(y)                                                          # n
k_max       = 4                                                                  # maximaler Polynomgrad
beta_hat    = list()                                                             # Parameterschätzerlisteninitialisierung
q           = rep(NaN,k_max)                                                     # q-Funktionswertinitialisierung

# Iteration über Ausgleichspolynome
for(k in 1:k_max){
  X = matrix(rep(1,n), nrow = n)                                                 # Design Matrix Initialisierung
  for(i in 1:k){
    X = cbind(X,x^i)                                                             # Polynomterme
  }
  beta_hat[[k]] = inv(t(X) %*% X) %*% t(X) %*% y                                 # Parameterschätzer
  q[k]          = t(y - X %*% beta_hat[[k]]) %*% (y - X %*% beta_hat[[k]])       # q Funktionswert
}


# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(2,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)

# Iterationen über Subplots
for(k in 1:k_max){

  # Datenwerte
  plot(
  D$x_i,
  D$y_i,
  pch         = 16,
  xlab        = "x",
  ylab        = "y",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = sprintf("k = %1.0f, q = %3.1f", k, q[k]))

  # Ausgleichspolynom
  pol         = polyval(rev(as.vector(beta_hat[[k]])), D$x_i)
  print(pol)
  lines(
  D$x_i,
  pol)

  # Speichern
  dev.copy2pdf(
  file        = file.path(fdir, "alm_1_ausgleichspolynom.pdf"),
  width       = 7,
  height      = 7)

}
```

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("1_Abbildungen/alm_1_ausgleichspolynom.pdf")
```
\vspace{-4mm}
\footnotesize
\center

$\bullet (x_i, y_i)$ \hspace{2mm} \textbf{---} $f_{\hat{\beta}}(x) = \sum_{i=0}^k \hat{\beta}_i x^{i}$

#
\setstretch{3}
\vfill
\large

Methode der kleinsten Quadrate

**Einfache lineare Regression**

Selbstkontrollfragen

\vfill

# Einfache lineare Regression
Motivation

\justifying
\small
Eine Ausgleichsgerade erlaubt Aussagen über unbeobachtete $y$ Werte für $x$ Werte.
Der Wert von $q(\hat{\beta})$ quantifiziert die Güte der Ausgleichsgeradenpassung.
Eine Ausgleichsgerade erlaubt allerdings nur implizite Aussagen über die mit der 
Anpassung verbundene Unsicherheit.

In der einfachen linearen Regression wird die Idee einer Ausgleichsgerade um eine
probabilistische Komponente (normalverteilte Fehlervariable) erweitert, um quantitative
Aussagen über die mit einer Ausgleichsgeradenanpassung verbundene Unsicherheit machen
zu können. Weiterhin erlaubt die einfache lineare Regression, einen Hypothesentest-
basierten Zugang zur Einschätzung der angepassten Parameterwerte $\hat{\beta}_0$ 
und $\hat{\beta}_1$.

Wir betrachten hier zunächst nur das probabilistische Modell der einfachen linearen
Regression sowie die auf ihm basierende Maximum Likelihood Schätzung der Parameter
$\beta_0$ und $\beta_1$. Die Bewertung von Parameterschätzerunsicherheit sowie
parameterzentrierte Hypothesentests behandeln wir an späterer Stelle zunächst
im Allgemeinen.

# Einfache lineare Regression
\small
\begin{definition}[Generatives Modell der einfachen linearen Regression]
Für $i = 1,...,n$ sei
\begin{equation}\label{eq:modell_generativ}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i}
\end{equation}
wobei
\begin{itemize}
\item $x_i \in \mathbb{R}$ fest vorgegebene sogenannte \textit{Prädiktorwerte} oder \textit{Regressorwerte} sind,
\item $\beta_0,\beta_1 \in \mathbb{R}$ wahre, aber unbekannte, Parameterwerte sind und
\item $\varepsilon_{i} \sim N(0,\sigma^2)$ unabhängige und identisch normalverteilte nicht-beobachtbare Zufallsvariablen mit wahrem, aber unbekanntem, Parameter $\sigma^2 > 0$ sind.
\end{itemize}
Dann heißt \eqref{eq:modell_generativ} \textit{Generatives Modell der einfachen linearen Regression}.
\end{definition}

\footnotesize
Bemerkungen

* Das Modell der einfachen linearen Regression hat drei Parameter, $\beta_0,\beta_1 \in \mathbb{R}$ und $\sigma^2>0$.

# Einfache lineare Regression
\small
\begin{theorem}[Normalverteilungsmodell der einfachen linearen Regression]
\normalfont
\justifying
Das generative Modell der einfachen linearen Regression
\begin{equation}\label{eq:modell}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i} \mbox{ mit } \varepsilon_i \sim N(0,\sigma^2) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
lässt sich äquivalent in der Form
\begin{equation}\label{eq:modell_normal}
Y_i \sim N\left(\beta_0 + \beta_1x_i, \sigma^2\right) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
schreiben.
\end{theorem}

\footnotesize

Bemerkungen

* Wir bezeichnen 
\begin{equation}
Y_i \sim N\left(\beta_0 + \beta_1x_i, \sigma^2\right) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
als \textit{Normalverteilungsmodell der einfachen linearen Regression}.


# Einfache lineare Regression
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setstretch{1.2}
\underline{Beweis}

Wir zeigen die Äquivalenz für ein $i$, die Unabhängigkeit der $Y_i$ zeigen wir
an späterer Stelle im Rahmen des Allgemeinen Linearen Modells. Die Äquivalenz
beider Modellformen für ein $i$ folgt direkt aus der Transformation
normalverteilter Zufallsvariablen durch linear-affine Funktionen (cf. (8)
Transformationen der Normalverteilung). Speziell gilt im vorliegenden Fall für
$\varepsilon_i \sim N(0,\sigma^2)$, dass
\begin{equation}
Y_i = f(\varepsilon_i)
\mbox{ mit }
f : \mathbb{R} \to \mathbb{R}, \varepsilon_i \mapsto f(\varepsilon_i) := \varepsilon_i + (\beta_0 + \beta_1x_i)
\end{equation}
Mit dem WDF Transformationstheorem bei linear-affinen Abbildungen folgt dann
\begin{align}
\begin{split}
p_{Y_i}(y_i)
& = \frac{1}{|1|} p_{\varepsilon_i}\left(\frac{y_i - \beta_0 - \beta_1x_i}{1} \right)       \\
& = N\left(x_i - \beta_0 - \beta_1x_i; 0, \sigma^2\right)                                   \\
& = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i - \beta_0 - \beta_1x_i - 0)^2 \right)    \\
& = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i - (\beta_0 + \beta_1x_i)^2 \right)        \\
& = N\left(x_i; \beta_0 + \beta_1x_i,\sigma^2\right),
\end{split}
\end{align}
also
\begin{equation}
Y_i \sim N\left(\beta_0 + \beta_1x_i,\sigma^2\right).
\end{equation}



# Einfache lineare Regression
Modell der einfachen linearen Regression
\vspace{1mm}
\setstretch{1.2}

```{r, echo = F, eval = F}
# Deterministischer Modellteil
n           = 10                              # Anzahl Regressor/Datenwerte
x           = 1:n                             # Regressorwerte
beta_0      = 0                               # Offsetparameter
beta_1      = 1                               # Slopeparameter
sigsqr      = 1                               # Varianzparameter
mu          = beta_0 + beta_1*x               # Normalverteilungserwartungswertparameter

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Deterministischer Modellteil
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
mu,
type        = "b",
lty         = 2,
pch         = 16,
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)

# Probabilistischer Modellteil
w           = 3                          # WDF Support Width
res         = 1e3                        # WDF resolution
for(i in 1:length(x)){
  y    = seq(mu[i]-w,mu[i]+w, len = res)
  pdfy = -dnorm(y, mu[i], sigsqr)
  lines(-pdfy+mu[i],y      , col = "gray80")
  lines(-rep(0,res)+mu[i],y, col = "gray40")
}

# Regressorwerte
points(
x,
rep(ylimits[1], n),
col = "blue",
pch = 16)


# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_1_elr_1.pdf"),
width       = 6,
height      = 4)

```


```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("1_Abbildungen/alm_1_elr_1.pdf")
```
\vspace{-4mm}
\footnotesize
\center

\textcolor{blue}{$\bullet$} $x_i$
\hspace{2mm}
$\bullet$ $\beta_0 + \beta_1x_i$ \mbox{ für } $\beta_0 := 0$, $\beta_1 := 1$
\hspace{2mm}
\textcolor{gray}{\textbf{---}} $N(y_i; \beta_0 + \beta_1x_i, \sigma^2)$ \mbox{ für } $\sigma^2 := 1$.

# Einfache lineare Regression
Realisierung des Modells der einfachen linearen Regression
\vspace{1mm}
\setstretch{1.2}

```{r, echo = F, eval = F}
# Deterministischer Modellteil
n           = 10                              # Anzahl Regressor/Datenwerte
x           = 1:n                             # Regressorwerte
beta_0      = 0                               # Offsetparameter
beta_1      = 1                               # Slopeparameter
sigsqr      = 1                               # Varianzparameter
mu          = beta_0 + beta_1*x               # Normalverteilungserwartungswertparameter

# Datenrealisierung
y           = rep(NaN,n)                      # Datenarrayinitialisierung
for(i in 1:n){
  y[i] = rnorm(1,mu[i],sigsqr)                # Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)
}


# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Deterministischer Modellteil
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
mu,
type        = "b",
lty         = 2,
pch         = 16,
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)

# Probabilistischer Modellteil
w           = 3                          # WDF Support Width
res         = 1e3                        # WDF resolution
for(i in 1:length(x)){
  yp   = seq(mu[i]-w,mu[i]+w, len = res)
  pdfy = -dnorm(yp, mu[i], sigsqr)
  lines(-pdfy+mu[i],yp      , col = "gray80")
  lines(-rep(0,res)+mu[i],yp, col = "gray40")
}

# Regressorwerte
points(
x,
rep(ylimits[1], n),
col = "blue",
pch = 16)

# Datenwerte
points(
x,
y,
col = "red",
pch = 16)


# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_1_elr_2.pdf"),
width       = 6,
height      = 4)

```


```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("1_Abbildungen/alm_1_elr_2.pdf")
```
\vspace{-4mm}
\footnotesize
\center

\textcolor{blue}{$\bullet$} $x_i$
\hspace{2mm}
$\bullet$ $\beta_0 + \beta_1x_i$ \mbox{ für } $\beta_0 := 0$, $\beta_1 := 1$
\hspace{2mm}
\textcolor{gray}{\textbf{---}} $N(y_i; \beta_0 + \beta_1x_i, \sigma^2)$ \mbox{ für } $\sigma^2 := 1$
\hspace{2mm}
\textcolor{red}{$\bullet$} $(x_i,y_i)$


# Einfache lineare Regression
\small
\begin{theorem}[Maximum Likelihood Schätzung]
\justifying
\normalfont
Es sei
\begin{equation}\label{eq:modell}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i} \mbox{ mit } \varepsilon_i \sim N(0,\sigma^2) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
das Modell der einfachen linearen Regression. Dann sind Maximum Likelihood
Schätzer der Modellparameter $\beta_0,\beta_1$ und $\sigma^2$ gegeben durch
\begin{equation}
\hat{\beta}_1  := \frac{c_{xy}}{s_x^2}, \,\,\,
\hat{\beta}_0  := \bar{y} - \hat{\beta}_1\bar{x} \,\,\,
\mbox{ und }
\hat{\sigma}^2 := \frac{1}{n}\sum_{i=1}^n \left(y_i - \left(\hat{\beta}_0 + \hat{\beta}_1 x_i\right)\right)^2.
\end{equation}
\end{theorem}

\footnotesize
Bemerkungen

* Wir verzichten hier aus Gründen der Übersichtlichkeit auf die $^{\tiny \mbox{ML}}$ Superskripte.
* Die ML Schätzer für $\beta_0$ und $\beta_1$ sind offenbar mit den Ausgleichsgeradenparametern identisch.


# Einfache lineare Regression
\tiny
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\underline{Beweis}

Wir zeigen zunächst, dass die Ausgleichsgerardenparameter $\hat{\beta}_0$ und
$\hat{\beta}_1$ den entsprechenden ML Schätzern gleichen. Dazu halten wir zunächst fest,
dass aufgrund der Unabhängigkeit der $Y_1, ...,Y_n$ die Likelihood-Funktion des
Modells der einfachen linearen Regression bezüglich $\beta_0$ und $\beta_1$ die
Form
\begin{align}
\begin{split}
L : \mathbb{R}^2 \to \mathbb{R}_{>0}, (\beta_0,\beta_1) \mapsto L(\beta_0,\beta_1)
& := \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y_i - (\beta_0 + \beta_1x_i))^2\right) \\
&  = \left(2\pi\sigma^2\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - (\beta_0 + \beta_1x_i))^2\right).
\end{split}
\end{align}
Weil für die Exponentialfunktion gilt, dass für $a < b \le 0$ gilt, dass $\exp(a)<\exp(b)$
wird der Exponentialterm dieser Likelihood-Funktion maximal, wenn der Term
\begin{equation}\label{eq:q}
q := \sum_{i=1}^n(y_i - (\beta_0 + \beta_1x_i))^2 \ge 0
\end{equation}
minimal und damit $-q$ maximal wird. Im Rahmen des Beweises der Ausgleichsgeradenform
haben wir aber schon gezeigt, dass der Term \eqref{eq:q} für
\begin{equation}
\hat{\beta}_1   := \frac{c_{xy}}{s_x^2} \mbox{ und } \hat{\beta}_0   := \bar{y} - \hat{\beta}_1\bar{x}
\end{equation}
minimal wird, und damit $\hat{\beta}_1$ und $\hat{\beta}_0$ die Likelihood-Funktion
maximieren.


# Einfache lineare Regression
\tiny
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\underline{Beweis (fortgeführt)}

In einem zweiten Schritt betrachten wir nun die Likelihood-Funktion des Modells
der einfachen linearen Regression bezüglich $\sigma^2$ an der Stelle von $\hat{\beta}_0$
und $\hat{\beta}_1$. Wir erhalten die Likelihood-Funktion
\begin{equation}
L : \mathbb{R}_{>0} \to \mathbb{R}_{>0}, \sigma^2 \mapsto L(\sigma^2)
 = \left(2\pi\sigma^2\right)^{-\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))^2\right)
\end{equation}
und die entsprechende Log-Likelihood-Funktion
\begin{equation}
\ell : \mathbb{R}_{>0} \to \mathbb{R}, \sigma^2 \mapsto \ell(\sigma^2)
 = -\frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2 -\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))^2
\end{equation}
In Analogie zu der Herleitung des ML Schätzers für $\sigma^2$ im Normalverteilungsmodell
(cf. (10) Parameterschätzung) ergibt sich unter Beachtung von
\begin{equation}
\hat{\mu}^{\mbox{\tiny ML}}_n = \hat{\beta}_0 + \hat{\beta}_1x_i
\end{equation}
dann hier
\begin{equation}
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))^2.
\end{equation}


# Einfache lineare Regression
Beispieldatensatz Parameterschätzung
\vspace{2mm}
\setstretch{1}

\tiny
```{r}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Stichprobenstatistiken
n           = length(D$y_i)                                      # Anzahl Datenpunkte
x_bar       = mean(D$x_i)                                        # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)                                        # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                                         # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)                                  # Stichprobenkovarianz der (x_i,y_i)-Werte

# Parameteterschätzer
beta_1_hat  = cxy/s2x                                            # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar                           # \hat{\beta}_0, Offset Parameter
sigsqr_hat  = (1/n)*sum((D$y_i-(beta_0_hat+beta_1_hat*D$x_i))^2) # Varianzparameter


# Ausgabe
cat("beta_0_hat:"  , beta_0_hat,
    "\nbeta_1_hat:", beta_1_hat,
    "\nsigsqr_hat:", sqrt(sigsqr_hat))
```

# Einfache lineare Regression
Beispieldatensatz Analyse mit `lm()`
\vspace{1mm}
\setstretch{1.2}

\tiny
```{r}
# Einlesen des Beispieldatensatzes
library(car)
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Analyse mit lm()
model       = lm(formula = D$y_i ~ D$x_i, data = D)
print(model)
```

#
\setstretch{3}
\vfill
\large

Methode der kleinsten Quadrate

Einfache lineare Regression

**Selbstkontrollfragen**

\vfill


# References
\footnotesize


